# # main.py
# import datetime
#
# from BiliBliDanmu2.config import SENTIMENT_POSITIVE_THRESHOLD, SENTIMENT_NEUTRAL_LOWER_THRESHOLD
# from bilibili_api import get_cid, fetch_danmaku_segment_bytes
# from danmaku_parser import parse_danmaku_from_bytes
# from text_analyzer import clean_danmaku_list, load_stopwords, calculate_word_frequency, batch_analyze_sentiments # å¼•å…¥æ–°å‡½æ•°
# import os # å¼•å…¥osæ¨¡å—æ¥åˆ›å»ºç›®å½•
#
#
#
# def get_date_range(start_date_str: str, end_date_str: str) -> list[str]:
#     """ç”Ÿæˆæ—¥æœŸèŒƒå›´å†…çš„æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ (YYYY-MM-DD)"""
#     start_date = datetime.datetime.strptime(start_date_str, "%Y-%m-%d").date()
#     end_date = datetime.datetime.strptime(end_date_str, "%Y-%m-%d").date()
#
#     delta = end_date - start_date
#     dates = []
#     for i in range(delta.days + 1):
#         day = start_date + datetime.timedelta(days=i)
#         dates.append(day.strftime("%Y-%m-%d"))
#     return dates
#
#
# def main():
#     print("æ¬¢è¿ä½¿ç”¨Bç«™å†å²å¼¹å¹•è·å–å·¥å…·ï¼")
#
#     # 1. è·å–ç”¨æˆ·è¾“å…¥
#     bvid = input("è¯·è¾“å…¥Bç«™è§†é¢‘çš„BVå· (ä¾‹å¦‚ BV1GJ411x7h7): ").strip()
#     # !!! é‡è¦: Cookieæ¶‰åŠä¸ªäººéšç§ï¼Œå®é™…é¡¹ç›®ä¸­ä¸åº”ç¡¬ç¼–ç æˆ–æ˜æ–‡ä¼ è¾“/å­˜å‚¨ !!!
#     # è¿™é‡Œä¸ºäº†æ•™å­¦æ¼”ç¤ºï¼Œç›´æ¥è¾“å…¥ã€‚è¯·ç¡®ä¿ä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–SESSDATAçš„å€¼ã€‚
#     user_sessdata = input("è¯·è¾“å…¥ä½ çš„Bç«™SESSDATA Cookieå€¼: ").strip()
#     if not user_sessdata:
#         print("é”™è¯¯ï¼šSESSDATAä¸èƒ½ä¸ºç©ºï¼")
#         return
#     user_cookie = f"SESSDATA={user_sessdata}"  # æ„é€ æˆå®Œæ•´çš„Cookieå­—ç¬¦ä¸²
#
#     start_date_str = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ (æ ¼å¼ YYYY-MM-DD, ä¾‹å¦‚ 2023-01-01): ").strip()
#     end_date_str = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸ (æ ¼å¼ YYYY-MM-DD, ä¾‹å¦‚ 2023-01-03): ").strip()
#
#     try:
#         dates_to_fetch = get_date_range(start_date_str, end_date_str)
#     except ValueError:
#         print("æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ç¡®ä¿è¾“å…¥YYYY-MM-DDæ ¼å¼ã€‚")
#         return
#
#     if not bvid:
#         print("BVå·ä¸èƒ½ä¸ºç©ºï¼")
#         return
#
#     # 2. è·å–CID
#     print(f"\næ­£åœ¨ä¸ºè§†é¢‘ {bvid} è·å–CID...")
#     oid = get_cid(bvid, user_cookie)
#     if not oid:
#         print("æ— æ³•è·å–CIDï¼Œç¨‹åºç»ˆæ­¢ã€‚è¯·æ£€æŸ¥BVå·å’ŒCookieã€‚")
#         return
#
#     # 3. å¾ªç¯ä¸‹è½½å’Œè§£æå¼¹å¹•
#     all_danmaku_texts = []
#     print(f"\nå¼€å§‹è·å–ä» {start_date_str} åˆ° {end_date_str} çš„å¼¹å¹•...")
#     for date_str in dates_to_fetch:
#         danmaku_segment_bytes = fetch_danmaku_segment_bytes(oid, date_str, user_cookie)
#         if danmaku_segment_bytes:
#             parsed_texts = parse_danmaku_from_bytes(danmaku_segment_bytes)
#             if parsed_texts:
#                 print(f"æ—¥æœŸ {date_str}: æˆåŠŸè§£æ {len(parsed_texts)} æ¡å¼¹å¹•ã€‚")
#                 all_danmaku_texts.extend(parsed_texts)
#             else:
#                 print(f"æ—¥æœŸ {date_str}: æœªè§£æåˆ°å¼¹å¹•æˆ–è§£æå¤±è´¥ã€‚")
#         else:
#             print(f"æ—¥æœŸ {date_str}: ä¸‹è½½å¼¹å¹•æ•°æ®å¤±è´¥ã€‚")
#
#     # 4. åˆæ­¥æˆæœå±•ç¤º
#     if all_danmaku_texts:
#         print(f"\nğŸ‰ æˆåŠŸè·å–æ€»è®¡ {len(all_danmaku_texts)} æ¡å¼¹å¹•ï¼")
#         print("éƒ¨åˆ†å¼¹å¹•ç¤ºä¾‹:")
#         for i, text in enumerate(all_danmaku_texts[:5]):  # æ˜¾ç¤ºå‰5æ¡
#             print(f"  {i + 1}. {text}")
#
#         # [å¯é€‰] ä¿å­˜æ‰€æœ‰åŸå§‹å¼¹å¹•åˆ°æ–‡ä»¶
#         # output_filename = f"{bvid}_raw_danmaku_{start_date_str}_to_{end_date_str}.txt"
#         # try:
#         #     with open(output_filename, 'w', encoding='utf-8') as f:
#         #         for text in all_danmaku_texts:
#         #             f.write(text + '\n')
#         #     print(f"\næ‰€æœ‰åŸå§‹å¼¹å¹•å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_filename}")
#         # except IOError as e:
#         #     print(f"\nä¿å­˜å¼¹å¹•æ–‡ä»¶å¤±è´¥: {e}")
#
#     else:
#         print("\næœªèƒ½è·å–åˆ°ä»»ä½•å¼¹å¹•ã€‚è¯·æ£€æŸ¥æ—¥æœŸèŒƒå›´ã€è§†é¢‘æ˜¯å¦æœ‰å¼¹å¹•æˆ–Cookieæ˜¯å¦æœ‰æ•ˆã€‚")
#
#     # 4. æ¸…æ´—å¼¹å¹•æ•°æ® <--- æ–°å¢æ­¥éª¤
#     if not all_danmaku_texts:
#         print("\n[!] æœªèƒ½è·å–åˆ°ä»»ä½•å¼¹å¹•ï¼Œæ— æ³•è¿›è¡Œåç»­åˆ†æã€‚")
#         return
#
#     cleaned_danmaku = clean_danmaku_list(all_danmaku_texts)
#
#     if not cleaned_danmaku:
#         print("[!] æ¸…æ´—åæ²¡æœ‰å‰©ä½™æœ‰æ•ˆå¼¹å¹•ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
#         return
#
#     # æ‰“å°æ¸…æ´—åçš„ç¤ºä¾‹
#     print("\n[*] æ¸…æ´—åå¼¹å¹•ç¤ºä¾‹:")
#     for i, text in enumerate(cleaned_danmaku[:5]): # æ˜¾ç¤ºå‰5æ¡
#         print(f"  {i+1}. {text}")
#
#     # [å¯é€‰] ä¿å­˜æ¸…æ´—åçš„å¼¹å¹•åˆ°æ–‡ä»¶
#     # save_cleaned = input("\næ˜¯å¦å°†æ¸…æ´—åçš„å¼¹å¹•ä¿å­˜åˆ°æ–‡ä»¶? (yes/noï¼Œé»˜è®¤ä¸ºno): ").strip().lower()
#     # if save_cleaned == 'yes':
#     #     cleaned_filename = f"{bvid}_cleaned_danmaku_{start_date_str}_to_{end_date_str}.txt"
#     #     try:
#     #         with open(cleaned_filename, 'w', encoding='utf-8') as f:
#     #             for text in cleaned_danmaku:
#     #                 f.write(text + '\n')
#     #         print(f"[*] æ¸…æ´—åçš„å¼¹å¹•å·²ä¿å­˜åˆ°æ–‡ä»¶: {cleaned_filename}")
#     #     except IOError as e:
#     #         print(f"[!] ä¿å­˜æ¸…æ´—åå¼¹å¹•æ–‡ä»¶å¤±è´¥: {e}")
#
#
#     # 5. åŠ è½½åœç”¨è¯å¹¶è¿›è¡Œè¯é¢‘ç»Ÿè®¡ <--- æ–°å¢æ­¥éª¤
#     stopwords = load_stopwords("cn_stopwords.txt") # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
#     word_frequency = calculate_word_frequency(cleaned_danmaku, stopwords)
#
#     if not word_frequency:
#         print("[!] æœªèƒ½ç»Ÿè®¡å‡ºè¯é¢‘ä¿¡æ¯ã€‚")
#         # è¿™é‡Œå¯ä»¥é€‰æ‹©æ˜¯å¦ç»ˆæ­¢ï¼Œæˆ–è€…ç»§ç»­è¿›è¡Œæƒ…æ„Ÿåˆ†æ
#         # return
#
#     # [å¯é€‰] ä¿å­˜è¯é¢‘ç»“æœåˆ°æ–‡ä»¶
#     # save_freq = input("\næ˜¯å¦å°†è¯é¢‘ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶? (yes/noï¼Œé»˜è®¤ä¸ºno): ").strip().lower()
#     # if save_freq == 'yes' and word_frequency:
#     #     freq_filename = f"{bvid}_word_frequency_{start_date_str}_to_{end_date_str}.csv"
#     #     try:
#     #         import csv
#     #         with open(freq_filename, 'w', encoding='utf-8', newline='') as f:
#     #             writer = csv.writer(f)
#     #             writer.writerow(['è¯è¯­', 'é¢‘æ¬¡']) # å†™å…¥è¡¨å¤´
#     #             writer.writerows(word_frequency)
#     #         print(f"[*] è¯é¢‘ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {freq_filename}")
#     #     except Exception as e:
#     #         print(f"[!] ä¿å­˜è¯é¢‘æ–‡ä»¶å¤±è´¥: {e}")
#     # --- åç»­çš„æƒ…æ„Ÿåˆ†æå’Œå¯è§†åŒ–æ­¥éª¤å°†åœ¨è¿™é‡Œæ·»åŠ  ---
#
#     # 6. è¿›è¡Œæƒ…æ„Ÿåˆ†æ <--- æ–°å¢æ­¥éª¤
#     sentiment_scores, sentiment_stats = batch_analyze_sentiments(cleaned_danmaku)
#
#     # --- å¯é€‰ä¿å­˜ï¼šæƒ…æ„Ÿåˆ†æè¯¦æƒ…å’Œæ‘˜è¦ ---
#     save_sentiment = input("\næ˜¯å¦å°†æƒ…æ„Ÿåˆ†æç»“æœä¿å­˜åˆ°æ–‡ä»¶? (yes/noï¼Œé»˜è®¤ä¸ºno): ").strip().lower()
#     if save_sentiment == 'yes' and sentiment_scores:
#         # åˆ›å»ºè¾“å‡ºç›®å½• (å¦‚æœä¸å­˜åœ¨)
#         output_dir = "output"
#         os.makedirs(output_dir, exist_ok=True)
#
#         # ä¿å­˜è¯¦ç»†æƒ…æ„Ÿåˆ†æ•° (å¼¹å¹•åŸæ–‡+åˆ†æ•°ï¼Œå¯èƒ½æ¯”è¾ƒå¤§)
#         sentiment_detail_filename = os.path.join(output_dir, f"{bvid}_sentiment_details_{start_date_str}_to_{end_date_str}.csv")
#         try:
#             import csv
#             with open(sentiment_detail_filename, 'w', encoding='utf-8', newline='') as f:
#                 writer = csv.writer(f)
#                 writer.writerow(['Danmaku', 'SentimentScore'])
#                 # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ cleaned_danmaku å’Œ sentiment_scores ä¸€ä¸€å¯¹åº”
#                 # ä½†ç”±äº analyze_sentiment å¯èƒ½è¿”å› Noneï¼Œç›´æ¥ zip å¯èƒ½ä¸å‡†ç¡®
#                 # æ›´å‡†ç¡®çš„åšæ³•æ˜¯åœ¨ batch_analyze_sentiments ä¸­åŒæ—¶è®°å½•æ–‡æœ¬å’Œåˆ†æ•°
#                 # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œå¯ä»¥åªä¿å­˜åˆ†æ•°åˆ—è¡¨ï¼Œæˆ–ä¿å­˜æ‘˜è¦
#             # print(f"[*] æƒ…æ„Ÿåˆ†æè¯¦æƒ…å·²å°è¯•ä¿å­˜åˆ°: {sentiment_detail_filename}") # ç®€åŒ–å¤„ç†
#         except Exception as e:
#             print(f"[!] ä¿å­˜æƒ…æ„Ÿè¯¦æƒ…æ–‡ä»¶å¤±è´¥: {e}")
#
#         # ä¿å­˜æƒ…æ„Ÿåˆ†ææ‘˜è¦
#         sentiment_summary_filename = os.path.join(output_dir, f"{bvid}_sentiment_summary_{start_date_str}_to_{end_date_str}.txt")
#         try:
#             with open(sentiment_summary_filename, 'w', encoding='utf-8') as f:
#                 total_valid = sentiment_stats["total_analyzed"]
#                 positive_count = sentiment_stats["positive"]
#                 neutral_count = sentiment_stats["neutral"]
#                 negative_count = sentiment_stats["negative"]
#                 f.write(f"Bç«™è§†é¢‘ {bvid} ({start_date_str} to {end_date_str}) å¼¹å¹•æƒ…æ„Ÿåˆ†ææ‘˜è¦\n")
#                 f.write("="*40 + "\n")
#                 f.write(f"æ€»è®¡åˆ†ææœ‰æ•ˆå¼¹å¹•æ•°: {total_valid}\n")
#                 if total_valid > 0:
#                     avg_score = sum(sentiment_scores) / total_valid if sentiment_scores else 0
#                     f.write(f"å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {avg_score:.4f}\n")
#                     f.write(f"ç§¯æå¼¹å¹• (>= {SENTIMENT_POSITIVE_THRESHOLD}): {positive_count} æ¡ ({positive_count/total_valid:.1%})\n")
#                     f.write(f"ä¸­æ€§å¼¹å¹• ({SENTIMENT_NEUTRAL_LOWER_THRESHOLD} ~ {SENTIMENT_POSITIVE_THRESHOLD}): {neutral_count} æ¡ ({neutral_count/total_valid:.1%})\n")
#                     f.write(f"æ¶ˆæå¼¹å¹• (< {SENTIMENT_NEUTRAL_LOWER_THRESHOLD}): {negative_count} æ¡ ({negative_count/total_valid:.1%})\n")
#                 else:
#                     f.write("æ— æœ‰æ•ˆå¼¹å¹•è¿›è¡Œç»Ÿè®¡ã€‚\n")
#             print(f"[*] æƒ…æ„Ÿåˆ†ææ‘˜è¦å·²ä¿å­˜åˆ°: {sentiment_summary_filename}")
#         except Exception as e:
#             print(f"[!] ä¿å­˜æƒ…æ„Ÿæ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
#
#     # --- åç»­çš„å¯è§†åŒ–æ­¥éª¤å°†åœ¨è¿™é‡Œæ·»åŠ  ---
#
#
#
# if __name__ == "__main__":
#     main()


# main.py (æœ€ç»ˆç‰ˆ)
import datetime
import os
from bilibili_api import get_cid, fetch_danmaku_segment_bytes
from danmaku_parser import parse_danmaku_from_bytes
# ç¡®ä¿å¯¼å…¥æ‰€æœ‰éœ€è¦çš„å‡½æ•°
from text_analyzer import (
    clean_danmaku_list, load_stopwords, calculate_word_frequency,
    batch_analyze_sentiments
)
# å¼•å…¥å¯è§†åŒ–å‡½æ•°
from visualizer import create_word_cloud, create_sentiment_distribution_chart

def get_date_range(start_date_str: str, end_date_str: str) -> list[str]:
    """ç”Ÿæˆæ—¥æœŸèŒƒå›´å†…çš„æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ (YYYY-MM-DD)"""
    start_date = datetime.datetime.strptime(start_date_str, "%Y-%m-%d").date()
    end_date = datetime.datetime.strptime(end_date_str, "%Y-%m-%d").date()
    delta = end_date - start_date
    dates = []
    for i in range(delta.days + 1):
        day = start_date + datetime.timedelta(days=i)
        dates.append(day.strftime("%Y-%m-%d"))
    return dates

def main():
    print("æ¬¢è¿ä½¿ç”¨Bç«™å†å²å¼¹å¹•åˆ†æå·¥å…·ï¼")

    # 1. è·å–ç”¨æˆ·è¾“å…¥
    bvid = input("è¯·è¾“å…¥Bç«™è§†é¢‘çš„BVå· (ä¾‹å¦‚ BV1GJ411x7h7): ").strip()
    user_sessdata = input("è¯·è¾“å…¥ä½ çš„Bç«™SESSDATA Cookieå€¼: ").strip()
    if not user_sessdata: print("é”™è¯¯ï¼šSESSDATAä¸èƒ½ä¸ºç©ºï¼"); return
    user_cookie = f"SESSDATA={user_sessdata}"
    start_date_str = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ (æ ¼å¼ YYYY-MM-DD): ").strip()
    end_date_str = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸ (æ ¼å¼ YYYY-MM-DD): ").strip()

    try:
        dates_to_fetch = get_date_range(start_date_str, end_date_str)
        date_range_str_for_filename = f"{start_date_str}_to_{end_date_str}" # ç”¨äºæ–‡ä»¶å
    except ValueError: print("æ—¥æœŸæ ¼å¼é”™è¯¯ã€‚"); return
    if not bvid: print("BVå·ä¸èƒ½ä¸ºç©ºï¼"); return

    # 2. è·å–CID
    print(f"\n[*] æ­£åœ¨ä¸ºè§†é¢‘ {bvid} è·å–CID...")
    oid = get_cid(bvid, user_cookie)
    if not oid: print("[!] æ— æ³•è·å–CIDï¼Œç¨‹åºç»ˆæ­¢ã€‚"); return

    # 3. å¾ªç¯ä¸‹è½½å’Œè§£æå¼¹å¹•
    all_danmaku_texts = []
    print(f"\n[*] å¼€å§‹è·å–ä» {start_date_str} åˆ° {end_date_str} çš„å¼¹å¹•...")
    # --- è¿™éƒ¨åˆ†å¾ªç¯ä¸‹è½½è§£æçš„ä»£ç ä¿æŒä¸å˜ ---
    for date_str in dates_to_fetch:
        danmaku_segment_bytes = fetch_danmaku_segment_bytes(oid, date_str, user_cookie)
        if danmaku_segment_bytes:
            parsed_texts = parse_danmaku_from_bytes(danmaku_segment_bytes)
            if parsed_texts:
                print(f"  [*] æ—¥æœŸ {date_str}: æˆåŠŸè§£æ {len(parsed_texts)} æ¡å¼¹å¹•ã€‚")
                all_danmaku_texts.extend(parsed_texts)
            else:
                # å³ä½¿è§£æä¸ºç©ºåˆ—è¡¨ï¼Œä¹Ÿè®¤ä¸ºæ˜¯â€œæˆåŠŸâ€ä¸‹è½½äº†æ–‡ä»¶ï¼ˆå¯èƒ½å½“å¤©æ²¡å¼¹å¹•æˆ–è§£æå™¨é—®é¢˜ï¼‰
                # print(f"  [!] æ—¥æœŸ {date_str}: æœªè§£æåˆ°å¼¹å¹•æˆ–è§£æå¤±è´¥ã€‚")
                pass # ä¸æ‰“å°ä¿¡æ¯ï¼Œå‡å°‘å¹²æ‰°
        else:
            # ä¸‹è½½å¤±è´¥çš„æ¶ˆæ¯å·²åœ¨ fetch å‡½æ•°å†…éƒ¨æ‰“å°
            pass
    # --- å¾ªç¯ç»“æŸ ---

    if not all_danmaku_texts:
        print("\n[!] æœªèƒ½è·å–åˆ°ä»»ä½•å¼¹å¹•ï¼Œæ— æ³•è¿›è¡Œåç»­åˆ†æã€‚")
        return
    print(f"\n[*] æˆåŠŸè·å–æ€»è®¡ {len(all_danmaku_texts)} æ¡åŸå§‹å¼¹å¹•ã€‚")

    # 4. æ¸…æ´—å¼¹å¹•æ•°æ®
    cleaned_danmaku = clean_danmaku_list(all_danmaku_texts)
    if not cleaned_danmaku:
        print("[!] æ¸…æ´—åæ²¡æœ‰å‰©ä½™æœ‰æ•ˆå¼¹å¹•ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    # 5. åŠ è½½åœç”¨è¯å¹¶è¿›è¡Œè¯é¢‘ç»Ÿè®¡
    stopwords = load_stopwords("cn_stopwords.txt")
    word_frequency = calculate_word_frequency(cleaned_danmaku, stopwords)

    # 6. è¿›è¡Œæƒ…æ„Ÿåˆ†æ
    sentiment_scores, sentiment_stats = batch_analyze_sentiments(cleaned_danmaku)

    # --- å®šä¹‰è¾“å‡ºç›®å½• ---
    output_directory = f"output_{bvid}_{date_range_str_for_filename}"
    os.makedirs(output_directory, exist_ok=True)
    print(f"\n[*] æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åœ¨ç›®å½•: {output_directory}")

    # 7. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ <--- æ–°å¢æ­¥éª¤
    # ç”Ÿæˆè¯äº‘å›¾ (å¦‚æœè¯é¢‘æ•°æ®å­˜åœ¨)
    if word_frequency:
        create_word_cloud(word_frequency, bvid, date_range_str_for_filename, output_dir=output_directory)

    # ç”Ÿæˆæƒ…æ„Ÿåˆ†å¸ƒå›¾ (å¦‚æœæƒ…æ„Ÿç»Ÿè®¡æ•°æ®æœ‰æ•ˆ)
    if sentiment_stats.get("total_analyzed", 0) > 0:
        create_sentiment_distribution_chart(sentiment_stats, bvid, date_range_str_for_filename,
                                            output_dir=output_directory)

    # --- å¯é€‰ä¿å­˜éƒ¨åˆ† (å¯ä»¥æ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Š) ---
    # if input("\næ˜¯å¦ä¿å­˜æ¸…æ´—åçš„å¼¹å¹•? (y/n): ").lower() == 'y':
    #     # ä¿å­˜ä»£ç ... (ä½¿ç”¨ os.path.join(output_directory, filename))
    # if input("æ˜¯å¦ä¿å­˜è¯é¢‘ç»“æœ? (y/n): ").lower() == 'y' and word_frequency:
    #     # ä¿å­˜ä»£ç ... (ä½¿ç”¨ os.path.join(output_directory, filename))
    # if input("æ˜¯å¦ä¿å­˜æƒ…æ„Ÿåˆ†ææ‘˜è¦? (y/n): ").lower() == 'y' and sentiment_stats['total_analyzed'] > 0:
    #     # ä¿å­˜ä»£ç ... (ä½¿ç”¨ os.path.join(output_directory, filename))

    print("\n--- åˆ†ææµç¨‹ç»“æŸ ---")


if __name__ == "__main__":
    main()